{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "common-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os, glob, unicodedata, string, random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-ex1",
   "metadata": {},
   "source": [
    "## Exercise 1: Natural Gas Price Prediction using RNN\n",
    "\n",
    "Dataset: Daily natural gas prices (nominal dollars) starting from January 1997. Given the last 10 days of prices, the model predicts the 11th day's price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-ex1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load & preprocess data\n",
    "df = pd.read_csv(\"./data/NaturalGasPrice/daily.csv\").dropna()\n",
    "y = df['Price'].values\n",
    "minm, maxm = y.min(), y.max()\n",
    "y_norm = (y - minm) / (maxm - minm)\n",
    "seq_len = 10\n",
    "X = np.array([y_norm[i:i+seq_len] for i in range(len(y_norm)-seq_len)])\n",
    "Y = np.array([y_norm[i+seq_len] for i in range(len(y_norm)-seq_len)])\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.1, random_state=42, shuffle=False)\n",
    "\n",
    "class NGTimeSeries(Dataset):\n",
    "    def __init__(self, x, y): self.x = torch.tensor(x, dtype=torch.float32); self.y = torch.tensor(y, dtype=torch.float32)\n",
    "    def __getitem__(self, idx): return self.x[idx], self.y[idx]\n",
    "    def __len__(self): return len(self.x)\n",
    "\n",
    "train_loader = DataLoader(NGTimeSeries(x_train,y_train), batch_size=256, shuffle=True)\n",
    "\n",
    "# Define RNN model\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self): \n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(1, 5, 1, batch_first=True)\n",
    "        self.fc = nn.Linear(5, 1)\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(-1)\n",
    "        out, _ = self.rnn(x)\n",
    "        return self.fc(torch.relu(out[:,-1,:]))\n",
    "\n",
    "model = RNNModel()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1500):\n",
    "    for xb, yb in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(model(xb).squeeze(), yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if epoch % 50 == 0: print(f\"Epoch {epoch} loss: {loss.item():.4f}\")\n",
    "\n",
    "# Evaluation & plots\n",
    "test_loader = DataLoader(NGTimeSeries(x_test, y_test), batch_size=len(x_test))\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader: \n",
    "        y_pred = model(xb).squeeze()\n",
    "plt.figure(figsize=(10,5)); plt.plot(y_pred.numpy(), label='Pred'); plt.plot(yb.numpy(), label='Orig'); plt.legend(); plt.show()\n",
    "y_pred_denorm = y_pred.numpy()*(maxm-minm)+minm\n",
    "yb_denorm = yb.numpy()*(maxm-minm)+minm\n",
    "plt.figure(figsize=(10,5)); plt.plot(yb_denorm, label='Orig'); plt.plot(y_pred_denorm, label='Pred'); plt.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-ex2",
   "metadata": {},
   "source": [
    "## Exercise 2: Name Classification with RNN\n",
    "\n",
    "Dataset: A collection of names stored in files (each file corresponds to a language). Train an RNN on surnames from 18 languages to predict the language based on name spelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-ex2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "all_letters = string.ascii_letters + \" .,;'\"\n",
    "n_letters = len(all_letters)\n",
    "def unicodeToAscii(s): return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn' and c in all_letters)\n",
    "def readLines(filename): return [unicodeToAscii(line) for line in open(filename, encoding='utf-8').read().strip().split('\\n')]\n",
    "categories, cat_lines = [], {}\n",
    "for fn in glob.glob('./data/names/*.txt'):\n",
    "    cat = os.path.splitext(os.path.basename(fn))[0]\n",
    "    categories.append(cat)\n",
    "    cat_lines[cat] = readLines(fn)\n",
    "if not categories: raise RuntimeError(\"No data in ./data/names/\")\n",
    "def nameToTensor(name):\n",
    "    tensor = torch.zeros(len(name), 1, n_letters)\n",
    "    for i, ch in enumerate(name): tensor[i][0][all_letters.find(ch)] = 1\n",
    "    return tensor\n",
    "\n",
    "# Define RNN classifier\n",
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, 1)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(1,1,hidden_size)\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        return self.fc(out[-1])\n",
    "hidden_size = 128; n_categories = len(categories)\n",
    "model = RNNClassifier(n_letters, hidden_size, n_categories)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.005)\n",
    "\n",
    "# Training loop\n",
    "def randomExample():\n",
    "    cat = random.choice(categories)\n",
    "    line = random.choice(cat_lines[cat])\n",
    "    return torch.tensor([categories.index(cat)], dtype=torch.long), nameToTensor(line), cat, line\n",
    "\n",
    "for i in range(100000):\n",
    "    cat_tensor, line_tensor, cat, line = randomExample()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(line_tensor)\n",
    "    loss = criterion(output, cat_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i % 5000 == 0:\n",
    "        guess = categories[torch.argmax(output)]\n",
    "        print(f\"{i}: {loss.item():.4f} {line} => {guess} ({cat})\")\n",
    "\n",
    "def predict(name, topk=3):\n",
    "    with torch.no_grad():\n",
    "        output = model(nameToTensor(name))\n",
    "        topv, topi = output.topk(topk)\n",
    "        return [(categories[topi[i].item()], topv[i].item()) for i in range(topk)]\n",
    "print(\"\\nPrediction for 'Satoshi':\", predict(\"Satoshi\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-ex3",
   "metadata": {},
   "source": [
    "## Exercise 3: Next Character Prediction using RNN\n",
    "\n",
    "Dataset: A text file (e.g., `./data/input.txt`). Train an RNN to predict the next character in a sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-ex3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load & prepare text data\n",
    "with open('./data/input.txt', 'r', encoding='utf-8') as f: text = f.read()\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "seq_len = 50; hidden_size = 128; num_layers = 1; lr = 0.003; epochs = 20; bs = 64\n",
    "sequences = [ [char_to_idx[ch] for ch in text[i:i+seq_len]] for i in range(len(text)-seq_len) ]\n",
    "targets = [ char_to_idx[text[i+seq_len]] for i in range(len(text)-seq_len) ]\n",
    "sequences, targets = np.array(sequences), np.array(targets)\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, seq, tgt): self.seq, self.tgt = seq, tgt\n",
    "    def __len__(self): return len(self.seq)\n",
    "    def __getitem__(self, idx): return torch.tensor(self.seq[idx], dtype=torch.long), torch.tensor(self.tgt[idx], dtype=torch.long)\n",
    "\n",
    "ds = TextDataset(sequences, targets)\n",
    "dl = DataLoader(ds, batch_size=bs, shuffle=True)\n",
    "\n",
    "# Define next-character RNN\n",
    "class NextCharRNN(nn.Module):\n",
    "    def __init__(self, vocab, hidden, layers):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab, hidden)\n",
    "        self.rnn = nn.RNN(hidden, hidden, layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden, vocab)\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        out, _ = self.rnn(x)\n",
    "        return self.fc(out[:,-1,:])\n",
    "\n",
    "model = NextCharRNN(vocab_size, hidden_size, num_layers)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    loss_sum = 0\n",
    "    for xb, yb in dl:\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(model(xb), yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_sum += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{epochs} loss: {loss_sum/len(dl):.4f}\")\n",
    "\n",
    "# Text generation function\n",
    "def gen_text(model, start, length=100):\n",
    "    model.eval()\n",
    "    inp = torch.tensor([char_to_idx[ch] for ch in start], dtype=torch.long).unsqueeze(0)\n",
    "    out_text = start\n",
    "    for _ in range(length):\n",
    "        with torch.no_grad():\n",
    "            out = model(inp)\n",
    "            p = torch.softmax(out, dim=1).squeeze().cpu().numpy()\n",
    "            char_idx = np.random.choice(len(p), p=p)\n",
    "            out_text += idx_to_char[char_idx]\n",
    "            inp = torch.cat([inp[:,1:], torch.tensor([[char_idx]], dtype=torch.long)], dim=1)\n",
    "    return out_text\n",
    "\n",
    "print(\"\\nGenerated Text:\\n\", gen_text(model, \"The \", 200))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
